{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratulb/mojo_programming/blob/main/gpu_puzzles/naive_matmul_one_thread_per_row.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buOgxm25ONit"
      },
      "outputs": [],
      "source": [
        "!curl -ssL https://magic.modular.com/ | bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVZvyhRiONiw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PATH'] +=':/root/.modular/bin'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqFD0EK0ONiw"
      },
      "outputs": [],
      "source": [
        "!magic init gpu_puzzles --format mojoproject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3Ddb6GcONiw"
      },
      "outputs": [],
      "source": [
        "%cd gpu_puzzles/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "IaxB1auxONix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754abac7-3143-4662-9c3d-1de527116f26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting naive_matmul_one_thread_per_row.mojo\n"
          ]
        }
      ],
      "source": [
        "%%writefile naive_matmul_one_thread_per_row.mojo\n",
        "\n",
        "### Dumb matrix multiplication\n",
        "### Simulate the CPU-style matrix multiplication with 1 GPU thread per row\n",
        "\n",
        "from gpu.host import DeviceContext, HostBuffer\n",
        "from gpu import thread_idx, block_idx, block_dim\n",
        "import random\n",
        "from layout import Layout, LayoutTensor\n",
        "from memory import UnsafePointer, memcpy\n",
        "from python import Python, PythonObject\n",
        "from testing import assert_true\n",
        "\n",
        "alias ROWS_A = 64\n",
        "alias COLS_A = 16\n",
        "alias ROWS_B = 16\n",
        "alias COLS_B = 8\n",
        "alias ROWS_C = ROWS_A\n",
        "alias COLS_C = COLS_B\n",
        "\n",
        "alias MATRIX_MIN_ELEM = -5.0\n",
        "alias MATRIX_MAX_ELEM = 5.0\n",
        "\n",
        "alias dtype = DType.float32\n",
        "# Num threads per block\n",
        "alias THREADS = ROWS_C\n",
        "# Total numbers blocks in the grid\n",
        "alias BLOCKS = 1\n",
        "\n",
        "alias layout_a = Layout.row_major(ROWS_A, COLS_A)\n",
        "alias layout_b = Layout.row_major(ROWS_B, COLS_B)\n",
        "alias layout_c = Layout.row_major(ROWS_C, COLS_C)\n",
        "\n",
        "\n",
        "alias MatrixA = LayoutTensor[dtype, layout_a, MutableAnyOrigin]\n",
        "alias MatrixB = LayoutTensor[dtype, layout_b, MutableAnyOrigin]\n",
        "alias MatrixC = LayoutTensor[dtype, layout_c, MutableAnyOrigin]\n",
        "\n",
        "\n",
        "fn naive_matmul_one_thread_per_row[\n",
        "    a: Layout, b: Layout, c: Layout\n",
        "](A: MatrixA, B: MatrixB, C: MatrixC,):\n",
        "    var tid = block_idx.x * block_dim.x + thread_idx.x\n",
        "\n",
        "    if tid < ROWS_A:  # Each thread id `tid` is a row of A or C\n",
        "        for j in range(COLS_B):\n",
        "            for k in range(COLS_A):\n",
        "                C[tid, j] += A[tid, k] * B[k, j]\n",
        "\n",
        "\n",
        "# Initialize the matrix buffer with values in the range 0 to 100\n",
        "fn fill_buffer(buffer: HostBuffer[dtype]):\n",
        "    # Randomize\n",
        "    random.seed()\n",
        "    for i in range(len(buffer)):\n",
        "        buffer[i] = random.random_float64(\n",
        "            MATRIX_MIN_ELEM, MATRIX_MAX_ELEM\n",
        "        ).cast[dtype]()[0]\n",
        "\n",
        "\n",
        "fn main():\n",
        "    try:\n",
        "        ctx = DeviceContext()\n",
        "\n",
        "        buffer_a = ctx.enqueue_create_buffer[dtype](\n",
        "            ROWS_A * COLS_A\n",
        "        ).enqueue_fill(0.0)\n",
        "        buffer_b = ctx.enqueue_create_buffer[dtype](\n",
        "            ROWS_B * COLS_B\n",
        "        ).enqueue_fill(0.0)\n",
        "        buffer_c = ctx.enqueue_create_buffer[dtype](\n",
        "            ROWS_C * COLS_C\n",
        "        ).enqueue_fill(0.0)\n",
        "\n",
        "        with buffer_a.map_to_host() as h_buffer_a:\n",
        "            fill_buffer(h_buffer_a)\n",
        "\n",
        "        with buffer_b.map_to_host() as h_buffer_b:\n",
        "            fill_buffer(h_buffer_b)\n",
        "\n",
        "        matrix_a = MatrixA(buffer_a)\n",
        "        matrix_b = MatrixB(buffer_b)\n",
        "        matrix_c = MatrixC(buffer_c)\n",
        "\n",
        "        ctx.enqueue_function[\n",
        "            naive_matmul_one_thread_per_row[layout_a, layout_b, layout_c]\n",
        "        ](\n",
        "            matrix_a,\n",
        "            matrix_b,\n",
        "            matrix_c,\n",
        "            grid_dim=BLOCKS,\n",
        "            block_dim=THREADS,\n",
        "        )\n",
        "\n",
        "        ctx.synchronize()\n",
        "\n",
        "        with buffer_a.map_to_host() as h_buffer_a:\n",
        "            with buffer_b.map_to_host() as h_buffer_b:\n",
        "                with buffer_c.map_to_host() as h_buffer_c:\n",
        "                    assert_allclose(\n",
        "                        (ROWS_A, COLS_A, h_buffer_a),\n",
        "                        (ROWS_B, COLS_B, h_buffer_b),\n",
        "                        (ROWS_C, COLS_C, h_buffer_c),\n",
        "                    )\n",
        "\n",
        "    except e:\n",
        "        print(\"Prininting here: \", e)\n",
        "\n",
        "\n",
        "fn assert_allclose(\n",
        "    buff_a_with_dims: (Int, Int, HostBuffer[dtype]),\n",
        "    buff_b_with_dims: (Int, Int, HostBuffer[dtype]),\n",
        "    buff_c_with_dims: (Int, Int, HostBuffer[dtype]),\n",
        ") raises:\n",
        "    a_rows, a_cols, a_buff = buff_a_with_dims\n",
        "    matrix_a = reshape(to_ndarray(a_buff), a_rows, a_cols)\n",
        "\n",
        "    b_rows, b_cols, b_buff = buff_b_with_dims\n",
        "    matrix_b = reshape(to_ndarray(b_buff), b_rows, b_cols)\n",
        "\n",
        "    c_rows, c_cols, c_buff = buff_c_with_dims\n",
        "    matrix_c = reshape(to_ndarray(c_buff), c_rows, c_cols)\n",
        "    np = Python.import_module(\"numpy\")\n",
        "    assert_true(np.allclose(np.matmul(matrix_a, matrix_b), matrix_c))\n",
        "    print(\"Assertion was successful\")\n",
        "\n",
        "\n",
        "fn to_ndarray(buffer: HostBuffer[dtype]) raises -> PythonObject:\n",
        "    np = Python.import_module(\"numpy\")\n",
        "    ndarray = np.zeros(len(buffer), dtype=np.float32)\n",
        "    ndarray_ptr = ndarray_ptr[dtype](ndarray)\n",
        "    buffer_ptr = buffer.unsafe_ptr()\n",
        "    memcpy(ndarray_ptr, buffer_ptr, len(buffer))\n",
        "    return ndarray\n",
        "\n",
        "\n",
        "fn reshape(ndarray: PythonObject, rows: Int, cols: Int) raises -> PythonObject:\n",
        "    return ndarray.reshape(rows, cols)\n",
        "\n",
        "\n",
        "fn ndarray_ptr[\n",
        "    dtype: DType\n",
        "](ndarray: PythonObject) raises -> UnsafePointer[Scalar[dtype]]:\n",
        "    return ndarray.__array_interface__[\"data\"][0].unsafe_get_as_pointer[dtype]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2k9wkDaONiz",
        "outputId": "f97261f4-2cca-4ad9-a584-026b2e4da9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m⠁\u001b[0m                                                                               \r\u001b[2K\u001b[32m⠁\u001b[0m activating environment                                                        \r\u001b[2K\u001b[32m⠁\u001b[0m activating environment                                                        \r\u001b[2K\u001b[1m/content/gpu_puzzles/naive_matmul_one_thread_per_row.mojo:1:1: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mstruct 'HostBuffer' utilizes conformance to trait 'Copyable & Movable' but does not explicitly declare it (implicit conformance is deprecated)\n",
            "\u001b[0m\n",
            "\u001b[0;1;32m^\n",
            "\u001b[0mAssertion was successful\n"
          ]
        }
      ],
      "source": [
        "!magic run mojo naive_matmul_one_thread_per_row.mojo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSglX7bNONi0",
        "outputId": "fef20385-6628-4f19-e7e3-e03e9f8edcc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m⠁\u001b[0m                                                                               \r\u001b[2K\u001b[32m⠁\u001b[0m activating environment                                                        \r\u001b[2K\u001b[32m⠁\u001b[0m activating environment                                                        \r\u001b[2K\u001b[1mreformatted naive_matmul_one_thread_per_row.mojo\u001b[0m\n",
            "\n",
            "\u001b[1mAll done! ✨ 🍰 ✨\u001b[0m\n",
            "\u001b[34m\u001b[1m1 file \u001b[0m\u001b[1mreformatted\u001b[0m.\n"
          ]
        }
      ],
      "source": [
        "!magic run mojo format naive_matmul_one_thread_per_row.mojo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat naive_matmul_one_thread_per_row.mojo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2OxrAfbJYKc",
        "outputId": "674412bb-d773-477f-ac85-ed3cb29e5adf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Dumb matrix multiplication\n",
            "### Use one one GPU thread for each column of the output matrix\n",
            "\n",
            "from gpu.host import DeviceContext, HostBuffer\n",
            "from gpu import thread_idx, block_idx, block_dim\n",
            "import random\n",
            "from layout import Layout, LayoutTensor\n",
            "from memory import UnsafePointer, memcpy, stack_allocation\n",
            "from python import Python, PythonObject\n",
            "from testing import assert_true\n",
            "from algorithm import vectorize\n",
            "from sys import simdwidthof, strided_load\n",
            "\n",
            "\n",
            "alias ROWS_A = 9\n",
            "alias COLS_A = 17\n",
            "alias ROWS_B = 17\n",
            "alias COLS_B = 7\n",
            "alias ROWS_C = ROWS_A\n",
            "alias COLS_C = COLS_B\n",
            "\n",
            "alias MATRIX_MIN_ELEM = -5.0\n",
            "alias MATRIX_MAX_ELEM = 5.0\n",
            "\n",
            "alias dtype = DType.float32\n",
            "# Num threads per block\n",
            "alias THREADS = (5, 5)\n",
            "# Total numbers blocks in the grid\n",
            "alias BLOCKS = (\n",
            "    (COLS_C + THREADS[0] - 1) // THREADS[0],\n",
            "    (ROWS_C + THREADS[1] - 1) // THREADS[1],\n",
            ")\n",
            "\n",
            "alias layout_a = Layout.row_major(ROWS_A, COLS_A)\n",
            "alias layout_b = Layout.row_major(ROWS_B, COLS_B)\n",
            "alias layout_c = Layout.row_major(ROWS_C, COLS_C)\n",
            "\n",
            "\n",
            "alias MatrixA = LayoutTensor[dtype, layout_a, MutableAnyOrigin]\n",
            "alias MatrixB = LayoutTensor[dtype, layout_b, MutableAnyOrigin]\n",
            "alias MatrixC = LayoutTensor[dtype, layout_c, MutableAnyOrigin]\n",
            "alias Storage = LayoutTensor[\n",
            "    dtype, Layout.row_major(1, simdwidthof[dtype]()), MutableAnyOrigin\n",
            "]\n",
            "\n",
            "\n",
            "fn dumb_matmaul(A: MatrixA, B: MatrixB, C: MatrixC, store: Storage):\n",
            "    var i = block_idx.y * block_dim.y + thread_idx.y  # Rows\n",
            "    var j = block_idx.x * block_dim.x + thread_idx.x  # Colums\n",
            "    if i < ROWS_C and j < COLS_C:\n",
            "        tile = stack_allocation[ROWS_B, Scalar[dtype]]()\n",
            "        each_b_col = B.tile[ROWS_B, 1](0, j)\n",
            "        for k in range(ROWS_B):\n",
            "            tile[k] = each_b_col[k, 0][0]\n",
            "\n",
            "        @parameter\n",
            "        fn dotproduct[simd_width: Int](idx: Int):\n",
            "            C[i, j] += (\n",
            "                A.load[width=simd_width](i, idx)\n",
            "                * tile.load[width=simd_width](idx)\n",
            "            ).reduce_add()\n",
            "\n",
            "        vectorize[dotproduct, simdwidthof[dtype]()](ROWS_B)\n",
            "\n",
            "\n",
            "# Initialize the matrix buffer with values in the range 0 to 100\n",
            "fn fill_buffer(buffer: HostBuffer[dtype]):\n",
            "    # Randomize\n",
            "    random.seed()\n",
            "    for i in range(len(buffer)):\n",
            "        buffer[i] = random.random_float64(\n",
            "            MATRIX_MIN_ELEM, MATRIX_MAX_ELEM\n",
            "        ).cast[dtype]()[0]\n",
            "\n",
            "\n",
            "fn main():\n",
            "    try:\n",
            "        ctx = DeviceContext()\n",
            "\n",
            "        buffer_a = ctx.enqueue_create_buffer[dtype](\n",
            "            ROWS_A * COLS_A\n",
            "        ).enqueue_fill(0.0)\n",
            "        buffer_b = ctx.enqueue_create_buffer[dtype](\n",
            "            ROWS_B * COLS_B\n",
            "        ).enqueue_fill(0.0)\n",
            "        buffer_c = ctx.enqueue_create_buffer[dtype](\n",
            "            ROWS_C * COLS_C\n",
            "        ).enqueue_fill(0.0)\n",
            "\n",
            "        store = ctx.enqueue_create_buffer[dtype](\n",
            "            simdwidthof[dtype]()\n",
            "        ).enqueue_fill(0.0)\n",
            "\n",
            "        with buffer_a.map_to_host() as h_buffer_a:\n",
            "            fill_buffer(h_buffer_a)\n",
            "\n",
            "        with buffer_b.map_to_host() as h_buffer_b:\n",
            "            fill_buffer(h_buffer_b)\n",
            "\n",
            "        matrix_a = MatrixA(buffer_a)\n",
            "        matrix_b = MatrixB(buffer_b)\n",
            "        # matrix_b = buffer_b.unsafe_ptr()\n",
            "        matrix_c = MatrixC(buffer_c)\n",
            "        storage = Storage(store)\n",
            "\n",
            "        ctx.enqueue_function[dumb_matmaul](\n",
            "            matrix_a,\n",
            "            matrix_b,\n",
            "            matrix_c,\n",
            "            storage,\n",
            "            grid_dim=BLOCKS,\n",
            "            block_dim=THREADS,\n",
            "        )\n",
            "\n",
            "        ctx.synchronize()\n",
            "\n",
            "        with buffer_a.map_to_host() as h_buffer_a:\n",
            "            with buffer_b.map_to_host() as h_buffer_b:\n",
            "                with buffer_c.map_to_host() as h_buffer_c:\n",
            "                    assert_allclose(\n",
            "                        (ROWS_A, COLS_A, h_buffer_a),\n",
            "                        (ROWS_B, COLS_B, h_buffer_b),\n",
            "                        (ROWS_C, COLS_C, h_buffer_c),\n",
            "                    )\n",
            "\n",
            "    except e:\n",
            "        print(\"Prininting here: \", e)\n",
            "\n",
            "\n",
            "fn assert_allclose(\n",
            "    buff_a_with_dims: (Int, Int, HostBuffer[dtype]),\n",
            "    buff_b_with_dims: (Int, Int, HostBuffer[dtype]),\n",
            "    buff_c_with_dims: (Int, Int, HostBuffer[dtype]),\n",
            ") raises:\n",
            "    a_rows, a_cols, a_buff = buff_a_with_dims\n",
            "    matrix_a = reshape(to_ndarray(a_buff), a_rows, a_cols)\n",
            "\n",
            "    b_rows, b_cols, b_buff = buff_b_with_dims\n",
            "    matrix_b = reshape(to_ndarray(b_buff), b_rows, b_cols)\n",
            "\n",
            "    c_rows, c_cols, c_buff = buff_c_with_dims\n",
            "    matrix_c = reshape(to_ndarray(c_buff), c_rows, c_cols)\n",
            "    np = Python.import_module(\"numpy\")\n",
            "    assert_true(np.allclose(np.matmul(matrix_a, matrix_b), matrix_c))\n",
            "    print(\"Assertion was successful\")\n",
            "\n",
            "\n",
            "fn to_ndarray(buffer: HostBuffer[dtype]) raises -> PythonObject:\n",
            "    np = Python.import_module(\"numpy\")\n",
            "    ndarray = np.zeros(len(buffer), dtype=np.float32)\n",
            "    ndarray_ptr = ndarray_ptr[dtype](ndarray)\n",
            "    buffer_ptr = buffer.unsafe_ptr()\n",
            "    memcpy(ndarray_ptr, buffer_ptr, len(buffer))\n",
            "    return ndarray\n",
            "\n",
            "\n",
            "fn reshape(ndarray: PythonObject, rows: Int, cols: Int) raises -> PythonObject:\n",
            "    return ndarray.reshape(rows, cols)\n",
            "\n",
            "\n",
            "fn ndarray_ptr[\n",
            "    dtype: DType\n",
            "](ndarray: PythonObject) raises -> UnsafePointer[Scalar[dtype]]:\n",
            "    return ndarray.__array_interface__[\"data\"][0].unsafe_get_as_pointer[dtype]()\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}